{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploración del efecto de hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargar datos desde el práctico anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"process_dataset/train_data_scaled_robust.csv\")\n",
    "y_train = np.load(\"process_dataset/y_train.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hiperparámetros KNN\n",
    "\n",
    "- **n_neighbors** (`int`): Número de vecinos (default=`5`)\n",
    "\n",
    "- **weights**: Método para la ponderación de las contribuciones de los vecinos (default=`uniforme`)\n",
    "  - **Valores**:\n",
    "    - `uniforme`: Todos los puntos de cada vecindario tienen la misma ponderación.\n",
    "    - `distancia`: Los vecinos más cercanos a un punto de consulta tendrán mayor influencia que los vecinos más lejanos.\n",
    "    - `callable`: Función definida por el usuario.\n",
    "\n",
    "- **algorithm**: Algoritmo utilizado para calcular los vecinos más cercanos (default=`auto`)\n",
    "  - **Valores**: `{'auto', 'ball_tree', 'kd_tree', 'brute'}`\n",
    "\n",
    "- **leaf_size** (`int`): Tamaño de hoja que se pasa a `BallTree` o `KDTree` (default=`30`)\n",
    "\n",
    "- **p** (`int`): Parámetro de potencia para la métrica de Minkowski (default=`2`)\n",
    "  - **Valores**:\n",
    "    1. `1`: Distancia Manhattan.\n",
    "    2. `2`: Distancia Euclideana.\n",
    "    3. `3`: Distancia Minkowski.\n",
    "\n",
    "- **metric**: Métrica para calcular la distancia (default=`minkowski`)\n",
    "\n",
    "- **n_jobs** (`int`): Número de trabajos paralelos para la búsqueda de vecinos (default=`None`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando KNN con n_neighbors=2, weights=uniform, metric=euclidean\n",
      "Entrenando KNN con n_neighbors=2, weights=uniform, metric=manhattan\n",
      "Entrenando KNN con n_neighbors=2, weights=distance, metric=euclidean\n",
      "Entrenando KNN con n_neighbors=2, weights=distance, metric=manhattan\n",
      "Entrenando KNN con n_neighbors=3, weights=uniform, metric=euclidean\n",
      "Entrenando KNN con n_neighbors=3, weights=uniform, metric=manhattan\n",
      "Entrenando KNN con n_neighbors=3, weights=distance, metric=euclidean\n",
      "Entrenando KNN con n_neighbors=3, weights=distance, metric=manhattan\n",
      "Entrenando KNN con n_neighbors=5, weights=uniform, metric=euclidean\n",
      "Entrenando KNN con n_neighbors=5, weights=uniform, metric=manhattan\n",
      "Entrenando KNN con n_neighbors=5, weights=distance, metric=euclidean\n",
      "Entrenando KNN con n_neighbors=5, weights=distance, metric=manhattan\n",
      "Entrenando KNN con n_neighbors=7, weights=uniform, metric=euclidean\n",
      "Entrenando KNN con n_neighbors=7, weights=uniform, metric=manhattan\n",
      "Entrenando KNN con n_neighbors=7, weights=distance, metric=euclidean\n",
      "Entrenando KNN con n_neighbors=7, weights=distance, metric=manhattan\n",
      "Entrenando KNN con n_neighbors=9, weights=uniform, metric=euclidean\n",
      "Entrenando KNN con n_neighbors=9, weights=uniform, metric=manhattan\n",
      "Entrenando KNN con n_neighbors=9, weights=distance, metric=euclidean\n",
      "Entrenando KNN con n_neighbors=9, weights=distance, metric=manhattan\n",
      "Entrenando KNN con n_neighbors=11, weights=uniform, metric=euclidean\n",
      "Entrenando KNN con n_neighbors=11, weights=uniform, metric=manhattan\n",
      "Entrenando KNN con n_neighbors=11, weights=distance, metric=euclidean\n",
      "Entrenando KNN con n_neighbors=11, weights=distance, metric=manhattan\n",
      "Entrenando KNN con n_neighbors=15, weights=uniform, metric=euclidean\n",
      "Entrenando KNN con n_neighbors=15, weights=uniform, metric=manhattan\n",
      "Entrenando KNN con n_neighbors=15, weights=distance, metric=euclidean\n",
      "Entrenando KNN con n_neighbors=15, weights=distance, metric=manhattan\n",
      "Entrenando KNN con n_neighbors=20, weights=uniform, metric=euclidean\n",
      "Entrenando KNN con n_neighbors=20, weights=uniform, metric=manhattan\n",
      "Entrenando KNN con n_neighbors=20, weights=distance, metric=euclidean\n",
      "Entrenando KNN con n_neighbors=20, weights=distance, metric=manhattan\n"
     ]
    }
   ],
   "source": [
    "n_neighbors_values = [2, 3, 5, 7, 9, 11, 15, 20]  # Número de vecinos\n",
    "weights_values = ['uniform', 'distance']  # Tipo de ponderación\n",
    "metrics_values = ['euclidean', 'manhattan']  # Tipos de distancia\n",
    "results = []\n",
    "\n",
    "for n in n_neighbors_values:\n",
    "    for weight in weights_values:\n",
    "        for metric in metrics_values:\n",
    "            print(f\"Entrenando KNN con n_neighbors={n}, weights={weight}, metric={metric}\")\n",
    "            knn_model = KNeighborsClassifier(n_neighbors=n, weights=weight, metric=metric) # Entrenar modelos\n",
    "            \n",
    "            results.append({\n",
    "                \"n_neighbors\": n,\n",
    "                \"weights\": weight,\n",
    "                \"metric\": metric,\n",
    "                \"Accuracy\": cross_val_score(knn_model, train_data, y_train, cv=10, scoring=\"accuracy\").mean(),\n",
    "                \"Recall\": cross_val_score(knn_model, train_data, y_train, cv=10, scoring=\"recall\").mean(),\n",
    "                \"Precision\": cross_val_score(knn_model, train_data, y_train, cv=10, scoring=\"precision\").mean(),\n",
    "                \"F1\": cross_val_score(knn_model, train_data, y_train, cv=10, scoring=\"f1\").mean()\n",
    "            })\n",
    "\n",
    "results_knn = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>weights</th>\n",
       "      <th>metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>uniform</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.493805</td>\n",
       "      <td>0.220080</td>\n",
       "      <td>0.467005</td>\n",
       "      <td>0.298681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>uniform</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.496460</td>\n",
       "      <td>0.215140</td>\n",
       "      <td>0.474345</td>\n",
       "      <td>0.295689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>distance</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.492478</td>\n",
       "      <td>0.473908</td>\n",
       "      <td>0.483160</td>\n",
       "      <td>0.477815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>distance</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.499558</td>\n",
       "      <td>0.483812</td>\n",
       "      <td>0.491565</td>\n",
       "      <td>0.487358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>uniform</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.489602</td>\n",
       "      <td>0.468505</td>\n",
       "      <td>0.480768</td>\n",
       "      <td>0.474330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>uniform</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.496460</td>\n",
       "      <td>0.475243</td>\n",
       "      <td>0.487345</td>\n",
       "      <td>0.480998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>distance</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.489381</td>\n",
       "      <td>0.468054</td>\n",
       "      <td>0.480498</td>\n",
       "      <td>0.473961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>distance</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.496681</td>\n",
       "      <td>0.476140</td>\n",
       "      <td>0.487548</td>\n",
       "      <td>0.481553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>uniform</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.492920</td>\n",
       "      <td>0.467224</td>\n",
       "      <td>0.484266</td>\n",
       "      <td>0.475002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>uniform</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.505088</td>\n",
       "      <td>0.486533</td>\n",
       "      <td>0.497215</td>\n",
       "      <td>0.491431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>distance</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.493142</td>\n",
       "      <td>0.467673</td>\n",
       "      <td>0.484487</td>\n",
       "      <td>0.475339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>distance</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.505088</td>\n",
       "      <td>0.486981</td>\n",
       "      <td>0.497194</td>\n",
       "      <td>0.491640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7</td>\n",
       "      <td>uniform</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.490044</td>\n",
       "      <td>0.461843</td>\n",
       "      <td>0.480985</td>\n",
       "      <td>0.470700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>uniform</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.501770</td>\n",
       "      <td>0.484733</td>\n",
       "      <td>0.494014</td>\n",
       "      <td>0.488879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>distance</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.489602</td>\n",
       "      <td>0.460944</td>\n",
       "      <td>0.480504</td>\n",
       "      <td>0.470007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>distance</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.501991</td>\n",
       "      <td>0.485630</td>\n",
       "      <td>0.494283</td>\n",
       "      <td>0.489473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9</td>\n",
       "      <td>uniform</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.489381</td>\n",
       "      <td>0.456413</td>\n",
       "      <td>0.479830</td>\n",
       "      <td>0.467407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9</td>\n",
       "      <td>uniform</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.495575</td>\n",
       "      <td>0.467634</td>\n",
       "      <td>0.486942</td>\n",
       "      <td>0.476650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9</td>\n",
       "      <td>distance</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.489159</td>\n",
       "      <td>0.455963</td>\n",
       "      <td>0.479595</td>\n",
       "      <td>0.467063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9</td>\n",
       "      <td>distance</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.494912</td>\n",
       "      <td>0.466735</td>\n",
       "      <td>0.486251</td>\n",
       "      <td>0.475851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>11</td>\n",
       "      <td>uniform</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.490708</td>\n",
       "      <td>0.456405</td>\n",
       "      <td>0.481304</td>\n",
       "      <td>0.468177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>11</td>\n",
       "      <td>uniform</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.495133</td>\n",
       "      <td>0.466735</td>\n",
       "      <td>0.485927</td>\n",
       "      <td>0.475552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>11</td>\n",
       "      <td>distance</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.490044</td>\n",
       "      <td>0.455955</td>\n",
       "      <td>0.480582</td>\n",
       "      <td>0.467614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>11</td>\n",
       "      <td>distance</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.494912</td>\n",
       "      <td>0.466733</td>\n",
       "      <td>0.485712</td>\n",
       "      <td>0.475448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>15</td>\n",
       "      <td>uniform</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.490487</td>\n",
       "      <td>0.458656</td>\n",
       "      <td>0.481143</td>\n",
       "      <td>0.469136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>15</td>\n",
       "      <td>uniform</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.486947</td>\n",
       "      <td>0.446544</td>\n",
       "      <td>0.476747</td>\n",
       "      <td>0.460687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>15</td>\n",
       "      <td>distance</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.489823</td>\n",
       "      <td>0.458653</td>\n",
       "      <td>0.480439</td>\n",
       "      <td>0.468798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>15</td>\n",
       "      <td>distance</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.487168</td>\n",
       "      <td>0.447443</td>\n",
       "      <td>0.476985</td>\n",
       "      <td>0.461260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>20</td>\n",
       "      <td>uniform</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.500442</td>\n",
       "      <td>0.371030</td>\n",
       "      <td>0.489286</td>\n",
       "      <td>0.421373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>20</td>\n",
       "      <td>uniform</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.496460</td>\n",
       "      <td>0.377809</td>\n",
       "      <td>0.485126</td>\n",
       "      <td>0.424436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>20</td>\n",
       "      <td>distance</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.494912</td>\n",
       "      <td>0.458631</td>\n",
       "      <td>0.485374</td>\n",
       "      <td>0.470793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>20</td>\n",
       "      <td>distance</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_neighbors   weights     metric  Accuracy    Recall  Precision        F1\n",
       "0             2   uniform  euclidean  0.493805  0.220080   0.467005  0.298681\n",
       "1             2   uniform  manhattan  0.496460  0.215140   0.474345  0.295689\n",
       "2             2  distance  euclidean  0.492478  0.473908   0.483160  0.477815\n",
       "3             2  distance  manhattan  0.499558  0.483812   0.491565  0.487358\n",
       "4             3   uniform  euclidean  0.489602  0.468505   0.480768  0.474330\n",
       "5             3   uniform  manhattan  0.496460  0.475243   0.487345  0.480998\n",
       "6             3  distance  euclidean  0.489381  0.468054   0.480498  0.473961\n",
       "7             3  distance  manhattan  0.496681  0.476140   0.487548  0.481553\n",
       "8             5   uniform  euclidean  0.492920  0.467224   0.484266  0.475002\n",
       "9             5   uniform  manhattan  0.505088  0.486533   0.497215  0.491431\n",
       "10            5  distance  euclidean  0.493142  0.467673   0.484487  0.475339\n",
       "11            5  distance  manhattan  0.505088  0.486981   0.497194  0.491640\n",
       "12            7   uniform  euclidean  0.490044  0.461843   0.480985  0.470700\n",
       "13            7   uniform  manhattan  0.501770  0.484733   0.494014  0.488879\n",
       "14            7  distance  euclidean  0.489602  0.460944   0.480504  0.470007\n",
       "15            7  distance  manhattan  0.501991  0.485630   0.494283  0.489473\n",
       "16            9   uniform  euclidean  0.489381  0.456413   0.479830  0.467407\n",
       "17            9   uniform  manhattan  0.495575  0.467634   0.486942  0.476650\n",
       "18            9  distance  euclidean  0.489159  0.455963   0.479595  0.467063\n",
       "19            9  distance  manhattan  0.494912  0.466735   0.486251  0.475851\n",
       "20           11   uniform  euclidean  0.490708  0.456405   0.481304  0.468177\n",
       "21           11   uniform  manhattan  0.495133  0.466735   0.485927  0.475552\n",
       "22           11  distance  euclidean  0.490044  0.455955   0.480582  0.467614\n",
       "23           11  distance  manhattan  0.494912  0.466733   0.485712  0.475448\n",
       "24           15   uniform  euclidean  0.490487  0.458656   0.481143  0.469136\n",
       "25           15   uniform  manhattan  0.486947  0.446544   0.476747  0.460687\n",
       "26           15  distance  euclidean  0.489823  0.458653   0.480439  0.468798\n",
       "27           15  distance  manhattan  0.487168  0.447443   0.476985  0.461260\n",
       "28           20   uniform  euclidean  0.500442  0.371030   0.489286  0.421373\n",
       "29           20   uniform  manhattan  0.496460  0.377809   0.485126  0.424436\n",
       "30           20  distance  euclidean  0.494912  0.458631   0.485374  0.470793\n",
       "31           20  distance  manhattan  0.493584  0.465865   0.484838  0.474805"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hiperparámetros árboles de decisión\n",
    "\n",
    "- **criterion**: Función para medir la calidad de una división. (default=`gini`)\n",
    "    - **valores**: {`gini`, `entropy`, `log_loss`}\n",
    "- **splitter**: La estrategia utilizada para elegir la división en cada nodo. (default=”best”)\n",
    "    - **values**: best para elegir la mejor división, “random” para elegir la mejor división aleatoria \n",
    "- **max_depthint**, default=None\n",
    "- **min_samples_split** default=2\n",
    "- **min_samples_leaf**, default=1\n",
    "- **min_weight_fraction_leaf** default=0.0\n",
    "- **max_features** {“sqrt”, “log2”}, default=None\n",
    "- **random_state** default=None\n",
    "- **max_leaf_nodes** default=None\n",
    "- **min_impurity_decrease** default=0.0\n",
    "- **class_weight** default=None\n",
    "- **ccp_alphanon-negative**, default=0.0\n",
    "- **monotonic_cst** default=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando DT con criterion=gini, splitter=best, max_depth=3\n",
      "Entrenando DT con criterion=gini, splitter=best, max_depth=4\n",
      "Entrenando DT con criterion=gini, splitter=best, max_depth=5\n",
      "Entrenando DT con criterion=gini, splitter=best, max_depth=7\n",
      "Entrenando DT con criterion=gini, splitter=best, max_depth=9\n",
      "Entrenando DT con criterion=gini, splitter=best, max_depth=11\n",
      "Entrenando DT con criterion=gini, splitter=best, max_depth=15\n",
      "Entrenando DT con criterion=gini, splitter=random, max_depth=3\n",
      "Entrenando DT con criterion=gini, splitter=random, max_depth=4\n",
      "Entrenando DT con criterion=gini, splitter=random, max_depth=5\n",
      "Entrenando DT con criterion=gini, splitter=random, max_depth=7\n",
      "Entrenando DT con criterion=gini, splitter=random, max_depth=9\n",
      "Entrenando DT con criterion=gini, splitter=random, max_depth=11\n",
      "Entrenando DT con criterion=gini, splitter=random, max_depth=15\n",
      "Entrenando DT con criterion=entropy, splitter=best, max_depth=3\n",
      "Entrenando DT con criterion=entropy, splitter=best, max_depth=4\n",
      "Entrenando DT con criterion=entropy, splitter=best, max_depth=5\n",
      "Entrenando DT con criterion=entropy, splitter=best, max_depth=7\n",
      "Entrenando DT con criterion=entropy, splitter=best, max_depth=9\n",
      "Entrenando DT con criterion=entropy, splitter=best, max_depth=11\n",
      "Entrenando DT con criterion=entropy, splitter=best, max_depth=15\n",
      "Entrenando DT con criterion=entropy, splitter=random, max_depth=3\n",
      "Entrenando DT con criterion=entropy, splitter=random, max_depth=4\n",
      "Entrenando DT con criterion=entropy, splitter=random, max_depth=5\n",
      "Entrenando DT con criterion=entropy, splitter=random, max_depth=7\n",
      "Entrenando DT con criterion=entropy, splitter=random, max_depth=9\n",
      "Entrenando DT con criterion=entropy, splitter=random, max_depth=11\n",
      "Entrenando DT con criterion=entropy, splitter=random, max_depth=15\n",
      "Entrenando DT con criterion=log_loss, splitter=best, max_depth=3\n",
      "Entrenando DT con criterion=log_loss, splitter=best, max_depth=4\n",
      "Entrenando DT con criterion=log_loss, splitter=best, max_depth=5\n",
      "Entrenando DT con criterion=log_loss, splitter=best, max_depth=7\n",
      "Entrenando DT con criterion=log_loss, splitter=best, max_depth=9\n",
      "Entrenando DT con criterion=log_loss, splitter=best, max_depth=11\n",
      "Entrenando DT con criterion=log_loss, splitter=best, max_depth=15\n",
      "Entrenando DT con criterion=log_loss, splitter=random, max_depth=3\n",
      "Entrenando DT con criterion=log_loss, splitter=random, max_depth=4\n",
      "Entrenando DT con criterion=log_loss, splitter=random, max_depth=5\n",
      "Entrenando DT con criterion=log_loss, splitter=random, max_depth=7\n",
      "Entrenando DT con criterion=log_loss, splitter=random, max_depth=9\n",
      "Entrenando DT con criterion=log_loss, splitter=random, max_depth=11\n",
      "Entrenando DT con criterion=log_loss, splitter=random, max_depth=15\n"
     ]
    }
   ],
   "source": [
    "quality_type = ['gini', 'entropy', 'log_loss']\n",
    "splitter_type = ['best', 'random']  \n",
    "max_depth_values = [3, 4, 5, 7, 9, 11, 15] \n",
    "results = []\n",
    "\n",
    "for quality in quality_type:\n",
    "    for splitter in splitter_type:\n",
    "        for value in max_depth_values:\n",
    "            print(f\"Entrenando DT con criterion={quality}, splitter={splitter}, max_depth={value}\")\n",
    "            dt_model = DecisionTreeClassifier(criterion=quality, splitter=splitter, max_depth=value)\n",
    "\n",
    "            results.append({\n",
    "                'criterion': quality,\n",
    "                'splitter': splitter,\n",
    "                'max_depth': value,\n",
    "                \"Accuracy\": cross_val_score(knn_model, train_data, y_train, cv=10, scoring=\"accuracy\").mean(),\n",
    "                \"Recall\": cross_val_score(knn_model, train_data, y_train, cv=10, scoring=\"recall\").mean(),\n",
    "                \"Precision\": cross_val_score(knn_model, train_data, y_train, cv=10, scoring=\"precision\").mean(),\n",
    "                \"F1\": cross_val_score(knn_model, train_data, y_train, cv=10, scoring=\"f1\").mean()\n",
    "            })\n",
    "\n",
    "results_dt = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>criterion</th>\n",
       "      <th>splitter</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gini</td>\n",
       "      <td>best</td>\n",
       "      <td>3</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gini</td>\n",
       "      <td>best</td>\n",
       "      <td>4</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gini</td>\n",
       "      <td>best</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gini</td>\n",
       "      <td>best</td>\n",
       "      <td>7</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gini</td>\n",
       "      <td>best</td>\n",
       "      <td>9</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gini</td>\n",
       "      <td>best</td>\n",
       "      <td>11</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gini</td>\n",
       "      <td>best</td>\n",
       "      <td>15</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gini</td>\n",
       "      <td>random</td>\n",
       "      <td>3</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gini</td>\n",
       "      <td>random</td>\n",
       "      <td>4</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gini</td>\n",
       "      <td>random</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gini</td>\n",
       "      <td>random</td>\n",
       "      <td>7</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gini</td>\n",
       "      <td>random</td>\n",
       "      <td>9</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gini</td>\n",
       "      <td>random</td>\n",
       "      <td>11</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gini</td>\n",
       "      <td>random</td>\n",
       "      <td>15</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>entropy</td>\n",
       "      <td>best</td>\n",
       "      <td>3</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>entropy</td>\n",
       "      <td>best</td>\n",
       "      <td>4</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>entropy</td>\n",
       "      <td>best</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>entropy</td>\n",
       "      <td>best</td>\n",
       "      <td>7</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>entropy</td>\n",
       "      <td>best</td>\n",
       "      <td>9</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>entropy</td>\n",
       "      <td>best</td>\n",
       "      <td>11</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>entropy</td>\n",
       "      <td>best</td>\n",
       "      <td>15</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>entropy</td>\n",
       "      <td>random</td>\n",
       "      <td>3</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>entropy</td>\n",
       "      <td>random</td>\n",
       "      <td>4</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>entropy</td>\n",
       "      <td>random</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>entropy</td>\n",
       "      <td>random</td>\n",
       "      <td>7</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>entropy</td>\n",
       "      <td>random</td>\n",
       "      <td>9</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>entropy</td>\n",
       "      <td>random</td>\n",
       "      <td>11</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>entropy</td>\n",
       "      <td>random</td>\n",
       "      <td>15</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>best</td>\n",
       "      <td>3</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>best</td>\n",
       "      <td>4</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>best</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>best</td>\n",
       "      <td>7</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>best</td>\n",
       "      <td>9</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>best</td>\n",
       "      <td>11</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>best</td>\n",
       "      <td>15</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>random</td>\n",
       "      <td>3</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>random</td>\n",
       "      <td>4</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>random</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>random</td>\n",
       "      <td>7</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>random</td>\n",
       "      <td>9</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>random</td>\n",
       "      <td>11</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>random</td>\n",
       "      <td>15</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   criterion splitter  max_depth  Accuracy    Recall  Precision        F1\n",
       "0       gini     best          3  0.493584  0.465865   0.484838  0.474805\n",
       "1       gini     best          4  0.493584  0.465865   0.484838  0.474805\n",
       "2       gini     best          5  0.493584  0.465865   0.484838  0.474805\n",
       "3       gini     best          7  0.493584  0.465865   0.484838  0.474805\n",
       "4       gini     best          9  0.493584  0.465865   0.484838  0.474805\n",
       "5       gini     best         11  0.493584  0.465865   0.484838  0.474805\n",
       "6       gini     best         15  0.493584  0.465865   0.484838  0.474805\n",
       "7       gini   random          3  0.493584  0.465865   0.484838  0.474805\n",
       "8       gini   random          4  0.493584  0.465865   0.484838  0.474805\n",
       "9       gini   random          5  0.493584  0.465865   0.484838  0.474805\n",
       "10      gini   random          7  0.493584  0.465865   0.484838  0.474805\n",
       "11      gini   random          9  0.493584  0.465865   0.484838  0.474805\n",
       "12      gini   random         11  0.493584  0.465865   0.484838  0.474805\n",
       "13      gini   random         15  0.493584  0.465865   0.484838  0.474805\n",
       "14   entropy     best          3  0.493584  0.465865   0.484838  0.474805\n",
       "15   entropy     best          4  0.493584  0.465865   0.484838  0.474805\n",
       "16   entropy     best          5  0.493584  0.465865   0.484838  0.474805\n",
       "17   entropy     best          7  0.493584  0.465865   0.484838  0.474805\n",
       "18   entropy     best          9  0.493584  0.465865   0.484838  0.474805\n",
       "19   entropy     best         11  0.493584  0.465865   0.484838  0.474805\n",
       "20   entropy     best         15  0.493584  0.465865   0.484838  0.474805\n",
       "21   entropy   random          3  0.493584  0.465865   0.484838  0.474805\n",
       "22   entropy   random          4  0.493584  0.465865   0.484838  0.474805\n",
       "23   entropy   random          5  0.493584  0.465865   0.484838  0.474805\n",
       "24   entropy   random          7  0.493584  0.465865   0.484838  0.474805\n",
       "25   entropy   random          9  0.493584  0.465865   0.484838  0.474805\n",
       "26   entropy   random         11  0.493584  0.465865   0.484838  0.474805\n",
       "27   entropy   random         15  0.493584  0.465865   0.484838  0.474805\n",
       "28  log_loss     best          3  0.493584  0.465865   0.484838  0.474805\n",
       "29  log_loss     best          4  0.493584  0.465865   0.484838  0.474805\n",
       "30  log_loss     best          5  0.493584  0.465865   0.484838  0.474805\n",
       "31  log_loss     best          7  0.493584  0.465865   0.484838  0.474805\n",
       "32  log_loss     best          9  0.493584  0.465865   0.484838  0.474805\n",
       "33  log_loss     best         11  0.493584  0.465865   0.484838  0.474805\n",
       "34  log_loss     best         15  0.493584  0.465865   0.484838  0.474805\n",
       "35  log_loss   random          3  0.493584  0.465865   0.484838  0.474805\n",
       "36  log_loss   random          4  0.493584  0.465865   0.484838  0.474805\n",
       "37  log_loss   random          5  0.493584  0.465865   0.484838  0.474805\n",
       "38  log_loss   random          7  0.493584  0.465865   0.484838  0.474805\n",
       "39  log_loss   random          9  0.493584  0.465865   0.484838  0.474805\n",
       "40  log_loss   random         11  0.493584  0.465865   0.484838  0.474805\n",
       "41  log_loss   random         15  0.493584  0.465865   0.484838  0.474805"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hiperparámetros SVM\n",
    "- C\n",
    "- kernel{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’} or callable, default=’rbf’\n",
    "- degree, default=3\n",
    "- gamma{‘scale’, ‘auto’} or float, default=’scale’\n",
    "- coef0, default=0.0\n",
    "- shrinking, default=True\n",
    "- probability, default=False\n",
    "- tol, default=1e-3\n",
    "- cache_size, default=200\n",
    "- class_weight, default=None\n",
    "- verbose, default=False\n",
    "- max_iter, default=-1\n",
    "- decision_function_shape{‘ovo’, ‘ovr’}, default=’ovr’\n",
    "- break_ties, default=False\n",
    "- random_state, default=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando SVM con kernel=linear, decision_function=ovo\n",
      "Entrenando SVM con kernel=linear, decision_function=ovr\n",
      "Entrenando SVM con kernel=poly, decision_function=ovo\n",
      "Entrenando SVM con kernel=poly, decision_function=ovr\n",
      "Entrenando SVM con kernel=rbf, decision_function=ovo\n",
      "Entrenando SVM con kernel=rbf, decision_function=ovr\n",
      "Entrenando SVM con kernel=sigmoid, decision_function=ovo\n",
      "Entrenando SVM con kernel=sigmoid, decision_function=ovr\n",
      "Entrenando SVM con kernel=precomputed, decision_function=ovo\n",
      "Entrenando SVM con kernel=precomputed, decision_function=ovr\n",
      "Entrenando SVM con kernel=linear, decision_function=ovo\n",
      "Entrenando SVM con kernel=linear, decision_function=ovr\n",
      "Entrenando SVM con kernel=poly, decision_function=ovo\n",
      "Entrenando SVM con kernel=poly, decision_function=ovr\n",
      "Entrenando SVM con kernel=rbf, decision_function=ovo\n",
      "Entrenando SVM con kernel=rbf, decision_function=ovr\n",
      "Entrenando SVM con kernel=sigmoid, decision_function=ovo\n",
      "Entrenando SVM con kernel=sigmoid, decision_function=ovr\n",
      "Entrenando SVM con kernel=precomputed, decision_function=ovo\n",
      "Entrenando SVM con kernel=precomputed, decision_function=ovr\n",
      "Entrenando SVM con kernel=linear, decision_function=ovo\n",
      "Entrenando SVM con kernel=linear, decision_function=ovr\n",
      "Entrenando SVM con kernel=poly, decision_function=ovo\n",
      "Entrenando SVM con kernel=poly, decision_function=ovr\n",
      "Entrenando SVM con kernel=rbf, decision_function=ovo\n",
      "Entrenando SVM con kernel=rbf, decision_function=ovr\n",
      "Entrenando SVM con kernel=sigmoid, decision_function=ovo\n",
      "Entrenando SVM con kernel=sigmoid, decision_function=ovr\n",
      "Entrenando SVM con kernel=precomputed, decision_function=ovo\n",
      "Entrenando SVM con kernel=precomputed, decision_function=ovr\n",
      "Entrenando SVM con kernel=linear, decision_function=ovo\n",
      "Entrenando SVM con kernel=linear, decision_function=ovr\n",
      "Entrenando SVM con kernel=poly, decision_function=ovo\n",
      "Entrenando SVM con kernel=poly, decision_function=ovr\n",
      "Entrenando SVM con kernel=rbf, decision_function=ovo\n",
      "Entrenando SVM con kernel=rbf, decision_function=ovr\n",
      "Entrenando SVM con kernel=sigmoid, decision_function=ovo\n",
      "Entrenando SVM con kernel=sigmoid, decision_function=ovr\n",
      "Entrenando SVM con kernel=precomputed, decision_function=ovo\n",
      "Entrenando SVM con kernel=precomputed, decision_function=ovr\n",
      "Entrenando SVM con kernel=linear, decision_function=ovo\n",
      "Entrenando SVM con kernel=linear, decision_function=ovr\n",
      "Entrenando SVM con kernel=poly, decision_function=ovo\n",
      "Entrenando SVM con kernel=poly, decision_function=ovr\n",
      "Entrenando SVM con kernel=rbf, decision_function=ovo\n",
      "Entrenando SVM con kernel=rbf, decision_function=ovr\n",
      "Entrenando SVM con kernel=sigmoid, decision_function=ovo\n",
      "Entrenando SVM con kernel=sigmoid, decision_function=ovr\n",
      "Entrenando SVM con kernel=precomputed, decision_function=ovo\n",
      "Entrenando SVM con kernel=precomputed, decision_function=ovr\n",
      "Entrenando SVM con kernel=linear, decision_function=ovo\n",
      "Entrenando SVM con kernel=linear, decision_function=ovr\n",
      "Entrenando SVM con kernel=poly, decision_function=ovo\n",
      "Entrenando SVM con kernel=poly, decision_function=ovr\n",
      "Entrenando SVM con kernel=rbf, decision_function=ovo\n",
      "Entrenando SVM con kernel=rbf, decision_function=ovr\n",
      "Entrenando SVM con kernel=sigmoid, decision_function=ovo\n",
      "Entrenando SVM con kernel=sigmoid, decision_function=ovr\n",
      "Entrenando SVM con kernel=precomputed, decision_function=ovo\n",
      "Entrenando SVM con kernel=precomputed, decision_function=ovr\n",
      "Entrenando SVM con kernel=linear, decision_function=ovo\n",
      "Entrenando SVM con kernel=linear, decision_function=ovr\n",
      "Entrenando SVM con kernel=poly, decision_function=ovo\n",
      "Entrenando SVM con kernel=poly, decision_function=ovr\n",
      "Entrenando SVM con kernel=rbf, decision_function=ovo\n",
      "Entrenando SVM con kernel=rbf, decision_function=ovr\n",
      "Entrenando SVM con kernel=sigmoid, decision_function=ovo\n",
      "Entrenando SVM con kernel=sigmoid, decision_function=ovr\n",
      "Entrenando SVM con kernel=precomputed, decision_function=ovo\n",
      "Entrenando SVM con kernel=precomputed, decision_function=ovr\n"
     ]
    }
   ],
   "source": [
    "c_values = [3, 4, 5, 7, 9, 11, 15]\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed']\n",
    "decision_functions = ['ovo', 'ovr']  \n",
    "results = []\n",
    "\n",
    "for c in c_values:\n",
    "    for kernel in kernels:\n",
    "        for function in decision_functions:\n",
    "            print(f\"Entrenando SVM con kernel={kernel}, decision_function={function}\")\n",
    "            svm_model = SVC(C=c, kernel=kernel, decision_function_shape=function)\n",
    "            results.append({\n",
    "                \"C\": c,\n",
    "                \"kernel\": kernel,\n",
    "                \"decision_function\": function,\n",
    "                \"Accuracy\": cross_val_score(knn_model, train_data, y_train, cv=10, scoring=\"accuracy\").mean(),\n",
    "                \"Recall\": cross_val_score(knn_model, train_data, y_train, cv=10, scoring=\"recall\").mean(),\n",
    "                \"Precision\": cross_val_score(knn_model, train_data, y_train, cv=10, scoring=\"precision\").mean(),\n",
    "                \"F1\": cross_val_score(knn_model, train_data, y_train, cv=10, scoring=\"f1\").mean()\n",
    "            })\n",
    "\n",
    "results_svm = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>kernel</th>\n",
       "      <th>decision_function</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>linear</td>\n",
       "      <td>ovo</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>linear</td>\n",
       "      <td>ovr</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>poly</td>\n",
       "      <td>ovo</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>poly</td>\n",
       "      <td>ovr</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>rbf</td>\n",
       "      <td>ovo</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>15</td>\n",
       "      <td>rbf</td>\n",
       "      <td>ovr</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>15</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>ovo</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>15</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>ovr</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>15</td>\n",
       "      <td>precomputed</td>\n",
       "      <td>ovo</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>15</td>\n",
       "      <td>precomputed</td>\n",
       "      <td>ovr</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     C       kernel decision_function  Accuracy    Recall  Precision        F1\n",
       "0    3       linear               ovo  0.493584  0.465865   0.484838  0.474805\n",
       "1    3       linear               ovr  0.493584  0.465865   0.484838  0.474805\n",
       "2    3         poly               ovo  0.493584  0.465865   0.484838  0.474805\n",
       "3    3         poly               ovr  0.493584  0.465865   0.484838  0.474805\n",
       "4    3          rbf               ovo  0.493584  0.465865   0.484838  0.474805\n",
       "..  ..          ...               ...       ...       ...        ...       ...\n",
       "65  15          rbf               ovr  0.493584  0.465865   0.484838  0.474805\n",
       "66  15      sigmoid               ovo  0.493584  0.465865   0.484838  0.474805\n",
       "67  15      sigmoid               ovr  0.493584  0.465865   0.484838  0.474805\n",
       "68  15  precomputed               ovo  0.493584  0.465865   0.484838  0.474805\n",
       "69  15  precomputed               ovr  0.493584  0.465865   0.484838  0.474805\n",
       "\n",
       "[70 rows x 7 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hiperparámetros AdaBoost\n",
    "- estimator, default=None\n",
    "- n_estimators, default=50\n",
    "- learning_rate, default=1.0\n",
    "- algorithm{‘SAMME’, ‘SAMME.R’}, default=’SAMME.R’\n",
    "- random_state int, RandomState instance or None, default=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando AdaBoost con n_estimators=10, learning_rate=0.01, algorithm=SAMME\n",
      "Entrenando AdaBoost con n_estimators=10, learning_rate=0.01, algorithm=SAMME.R\n",
      "Entrenando AdaBoost con n_estimators=10, learning_rate=0.1, algorithm=SAMME\n",
      "Entrenando AdaBoost con n_estimators=10, learning_rate=0.1, algorithm=SAMME.R\n",
      "Entrenando AdaBoost con n_estimators=10, learning_rate=0.2, algorithm=SAMME\n",
      "Entrenando AdaBoost con n_estimators=10, learning_rate=0.2, algorithm=SAMME.R\n",
      "Entrenando AdaBoost con n_estimators=25, learning_rate=0.01, algorithm=SAMME\n",
      "Entrenando AdaBoost con n_estimators=25, learning_rate=0.01, algorithm=SAMME.R\n",
      "Entrenando AdaBoost con n_estimators=25, learning_rate=0.1, algorithm=SAMME\n",
      "Entrenando AdaBoost con n_estimators=25, learning_rate=0.1, algorithm=SAMME.R\n",
      "Entrenando AdaBoost con n_estimators=25, learning_rate=0.2, algorithm=SAMME\n",
      "Entrenando AdaBoost con n_estimators=25, learning_rate=0.2, algorithm=SAMME.R\n",
      "Entrenando AdaBoost con n_estimators=50, learning_rate=0.01, algorithm=SAMME\n",
      "Entrenando AdaBoost con n_estimators=50, learning_rate=0.01, algorithm=SAMME.R\n",
      "Entrenando AdaBoost con n_estimators=50, learning_rate=0.1, algorithm=SAMME\n",
      "Entrenando AdaBoost con n_estimators=50, learning_rate=0.1, algorithm=SAMME.R\n",
      "Entrenando AdaBoost con n_estimators=50, learning_rate=0.2, algorithm=SAMME\n",
      "Entrenando AdaBoost con n_estimators=50, learning_rate=0.2, algorithm=SAMME.R\n",
      "Entrenando AdaBoost con n_estimators=100, learning_rate=0.01, algorithm=SAMME\n",
      "Entrenando AdaBoost con n_estimators=100, learning_rate=0.01, algorithm=SAMME.R\n",
      "Entrenando AdaBoost con n_estimators=100, learning_rate=0.1, algorithm=SAMME\n",
      "Entrenando AdaBoost con n_estimators=100, learning_rate=0.1, algorithm=SAMME.R\n",
      "Entrenando AdaBoost con n_estimators=100, learning_rate=0.2, algorithm=SAMME\n",
      "Entrenando AdaBoost con n_estimators=100, learning_rate=0.2, algorithm=SAMME.R\n",
      "Entrenando AdaBoost con n_estimators=150, learning_rate=0.01, algorithm=SAMME\n",
      "Entrenando AdaBoost con n_estimators=150, learning_rate=0.01, algorithm=SAMME.R\n",
      "Entrenando AdaBoost con n_estimators=150, learning_rate=0.1, algorithm=SAMME\n",
      "Entrenando AdaBoost con n_estimators=150, learning_rate=0.1, algorithm=SAMME.R\n",
      "Entrenando AdaBoost con n_estimators=150, learning_rate=0.2, algorithm=SAMME\n",
      "Entrenando AdaBoost con n_estimators=150, learning_rate=0.2, algorithm=SAMME.R\n",
      "Entrenando AdaBoost con n_estimators=200, learning_rate=0.01, algorithm=SAMME\n",
      "Entrenando AdaBoost con n_estimators=200, learning_rate=0.01, algorithm=SAMME.R\n",
      "Entrenando AdaBoost con n_estimators=200, learning_rate=0.1, algorithm=SAMME\n",
      "Entrenando AdaBoost con n_estimators=200, learning_rate=0.1, algorithm=SAMME.R\n",
      "Entrenando AdaBoost con n_estimators=200, learning_rate=0.2, algorithm=SAMME\n",
      "Entrenando AdaBoost con n_estimators=200, learning_rate=0.2, algorithm=SAMME.R\n"
     ]
    }
   ],
   "source": [
    "n_estimators_values = [10, 25, 50, 100, 150, 200]  # Número de estimadores\n",
    "learning_rate_values = [0.01, 0.1, 0.2]  # Tasa de aprendizaje\n",
    "algorithms = ['SAMME', 'SAMME.R']\n",
    "results = []\n",
    "\n",
    "for n_estimators in n_estimators_values:\n",
    "    for learning_rate in learning_rate_values:\n",
    "        for alg in algorithms:\n",
    "            print(f\"Entrenando AdaBoost con n_estimators={n_estimators}, learning_rate={learning_rate}, algorithm={alg}\")\n",
    "            gb_model = AdaBoostClassifier(n_estimators=n_estimators, learning_rate=learning_rate, algorithm=alg, random_state=42)\n",
    "            \n",
    "            results.append({\n",
    "                'n_estimators': n_estimators,\n",
    "                'learning_rate': learning_rate,\n",
    "                'algorithm': alg,\n",
    "                \"Accuracy\": cross_val_score(knn_model, train_data, y_train, cv=10, scoring=\"accuracy\").mean(),\n",
    "                \"Recall\": cross_val_score(knn_model, train_data, y_train, cv=10, scoring=\"recall\").mean(),\n",
    "                \"Precision\": cross_val_score(knn_model, train_data, y_train, cv=10, scoring=\"precision\").mean(),\n",
    "                \"F1\": cross_val_score(knn_model, train_data, y_train, cv=10, scoring=\"f1\").mean()\n",
    "            })\n",
    "\n",
    "results_ada = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>50</td>\n",
       "      <td>0.20</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>50</td>\n",
       "      <td>0.20</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>100</td>\n",
       "      <td>0.10</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>100</td>\n",
       "      <td>0.10</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>100</td>\n",
       "      <td>0.20</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>100</td>\n",
       "      <td>0.20</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>150</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>150</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>150</td>\n",
       "      <td>0.10</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>150</td>\n",
       "      <td>0.10</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>150</td>\n",
       "      <td>0.20</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>150</td>\n",
       "      <td>0.20</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>200</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>200</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>200</td>\n",
       "      <td>0.10</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>200</td>\n",
       "      <td>0.10</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>200</td>\n",
       "      <td>0.20</td>\n",
       "      <td>SAMME</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>200</td>\n",
       "      <td>0.20</td>\n",
       "      <td>SAMME.R</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimators  learning_rate algorithm  Accuracy    Recall  Precision  \\\n",
       "0             10           0.01     SAMME  0.493584  0.465865   0.484838   \n",
       "1             10           0.01   SAMME.R  0.493584  0.465865   0.484838   \n",
       "2             10           0.10     SAMME  0.493584  0.465865   0.484838   \n",
       "3             10           0.10   SAMME.R  0.493584  0.465865   0.484838   \n",
       "4             10           0.20     SAMME  0.493584  0.465865   0.484838   \n",
       "5             10           0.20   SAMME.R  0.493584  0.465865   0.484838   \n",
       "6             25           0.01     SAMME  0.493584  0.465865   0.484838   \n",
       "7             25           0.01   SAMME.R  0.493584  0.465865   0.484838   \n",
       "8             25           0.10     SAMME  0.493584  0.465865   0.484838   \n",
       "9             25           0.10   SAMME.R  0.493584  0.465865   0.484838   \n",
       "10            25           0.20     SAMME  0.493584  0.465865   0.484838   \n",
       "11            25           0.20   SAMME.R  0.493584  0.465865   0.484838   \n",
       "12            50           0.01     SAMME  0.493584  0.465865   0.484838   \n",
       "13            50           0.01   SAMME.R  0.493584  0.465865   0.484838   \n",
       "14            50           0.10     SAMME  0.493584  0.465865   0.484838   \n",
       "15            50           0.10   SAMME.R  0.493584  0.465865   0.484838   \n",
       "16            50           0.20     SAMME  0.493584  0.465865   0.484838   \n",
       "17            50           0.20   SAMME.R  0.493584  0.465865   0.484838   \n",
       "18           100           0.01     SAMME  0.493584  0.465865   0.484838   \n",
       "19           100           0.01   SAMME.R  0.493584  0.465865   0.484838   \n",
       "20           100           0.10     SAMME  0.493584  0.465865   0.484838   \n",
       "21           100           0.10   SAMME.R  0.493584  0.465865   0.484838   \n",
       "22           100           0.20     SAMME  0.493584  0.465865   0.484838   \n",
       "23           100           0.20   SAMME.R  0.493584  0.465865   0.484838   \n",
       "24           150           0.01     SAMME  0.493584  0.465865   0.484838   \n",
       "25           150           0.01   SAMME.R  0.493584  0.465865   0.484838   \n",
       "26           150           0.10     SAMME  0.493584  0.465865   0.484838   \n",
       "27           150           0.10   SAMME.R  0.493584  0.465865   0.484838   \n",
       "28           150           0.20     SAMME  0.493584  0.465865   0.484838   \n",
       "29           150           0.20   SAMME.R  0.493584  0.465865   0.484838   \n",
       "30           200           0.01     SAMME  0.493584  0.465865   0.484838   \n",
       "31           200           0.01   SAMME.R  0.493584  0.465865   0.484838   \n",
       "32           200           0.10     SAMME  0.493584  0.465865   0.484838   \n",
       "33           200           0.10   SAMME.R  0.493584  0.465865   0.484838   \n",
       "34           200           0.20     SAMME  0.493584  0.465865   0.484838   \n",
       "35           200           0.20   SAMME.R  0.493584  0.465865   0.484838   \n",
       "\n",
       "          F1  \n",
       "0   0.474805  \n",
       "1   0.474805  \n",
       "2   0.474805  \n",
       "3   0.474805  \n",
       "4   0.474805  \n",
       "5   0.474805  \n",
       "6   0.474805  \n",
       "7   0.474805  \n",
       "8   0.474805  \n",
       "9   0.474805  \n",
       "10  0.474805  \n",
       "11  0.474805  \n",
       "12  0.474805  \n",
       "13  0.474805  \n",
       "14  0.474805  \n",
       "15  0.474805  \n",
       "16  0.474805  \n",
       "17  0.474805  \n",
       "18  0.474805  \n",
       "19  0.474805  \n",
       "20  0.474805  \n",
       "21  0.474805  \n",
       "22  0.474805  \n",
       "23  0.474805  \n",
       "24  0.474805  \n",
       "25  0.474805  \n",
       "26  0.474805  \n",
       "27  0.474805  \n",
       "28  0.474805  \n",
       "29  0.474805  \n",
       "30  0.474805  \n",
       "31  0.474805  \n",
       "32  0.474805  \n",
       "33  0.474805  \n",
       "34  0.474805  \n",
       "35  0.474805  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hiperparámetros GradientBoosting\n",
    "\n",
    "- **loss**: Función de pérdida a optimizar (default=`'log_loss'`).\n",
    "    - **Valores**: {`log_loss`, `exponencial`}. \n",
    "        - `log_loss`: Binomial o multinomial. \n",
    "        - `exponencial`: la optimización del modelo se comportará de manera similar a AdaBoost\n",
    "  \n",
    "- **learning_rate**: Tasa de aprendizaje para actualizar los pesos en cada iteración (default=`0.1`). Contribución de cada nuevo árbol que se añade al modelo\n",
    "\n",
    "- **n_estimators**: Número de estimadores (árboles) a utilizar (default=`100`).\n",
    "  \n",
    "- **subsample**: Fracción de muestras a utilizar para ajustar cada árbol (default=`1.0`).\n",
    "  \n",
    "- **criterion**: Criterio para la función de pérdida (default=`'friedman_mse'`).\n",
    "    - **Valores**: {‘friedman_mse’, ‘squared_error’}\n",
    "  \n",
    "- **min_samples_split**: Número mínimo de muestras necesarias para dividir un nodo (default=`2`).\n",
    "  \n",
    "- **min_samples_leaf**: Número mínimo de muestras en un nodo hoja (default=`1`).\n",
    "  \n",
    "- **min_weight_fraction_leaf**: Fracción mínima de peso requerida en un nodo hoja (default=`0.0`).\n",
    "  \n",
    "- **max_depth**: Profundidad máxima de los árboles (default=`3`).\n",
    "  \n",
    "- **min_impurity_decrease**: Umbral de disminución de impurezas para dividir un nodo (default=`0.0`).\n",
    "  \n",
    "- **init**: Estimador inicial para la predicción (default=`None`).\n",
    "  \n",
    "- **random_state** (`int` o `None`): Controla la aleatoriedad del muestreo y del modelo (default=`None`).\n",
    "  \n",
    "- **max_features** (`int`, `float`, o `None`): Número de características a considerar para buscar la mejor división (default=`None`).\n",
    "  \n",
    "- **verbose**: Controla el nivel de mensajes de salida (default=`0`).\n",
    "  \n",
    "- **max_leaf_nodes** (`int` o `None`): Número máximo de nodos hoja (default=`None`).\n",
    "  \n",
    "- **warm_start**: Si se deben reutilizar las soluciones anteriores para ajustarlas (default=`False`).\n",
    "  \n",
    "- **validation_fraction**: Fracción de muestras utilizadas para la validación en la parada temprana (default=`0.1`).\n",
    "  \n",
    "- **n_iter_no_change** (`int` o `None`): Número de iteraciones sin cambio en la validación para detener el entrenamiento (default=`None`).\n",
    "  \n",
    "- **tol**: Tolerancia para la parada temprana (default=`0.0001`).\n",
    "  \n",
    "- **ccp_alpha**: Parámetro de complejidad para la poda de costo-complejidad (default=`0.0`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando Gradient Boosting con n_estimators=10, learning_rate=0.01, max_depth=3\n",
      "Entrenando Gradient Boosting con n_estimators=10, learning_rate=0.01, max_depth=4\n",
      "Entrenando Gradient Boosting con n_estimators=10, learning_rate=0.01, max_depth=5\n",
      "Entrenando Gradient Boosting con n_estimators=10, learning_rate=0.1, max_depth=3\n",
      "Entrenando Gradient Boosting con n_estimators=10, learning_rate=0.1, max_depth=4\n",
      "Entrenando Gradient Boosting con n_estimators=10, learning_rate=0.1, max_depth=5\n",
      "Entrenando Gradient Boosting con n_estimators=10, learning_rate=0.2, max_depth=3\n",
      "Entrenando Gradient Boosting con n_estimators=10, learning_rate=0.2, max_depth=4\n",
      "Entrenando Gradient Boosting con n_estimators=10, learning_rate=0.2, max_depth=5\n",
      "Entrenando Gradient Boosting con n_estimators=25, learning_rate=0.01, max_depth=3\n",
      "Entrenando Gradient Boosting con n_estimators=25, learning_rate=0.01, max_depth=4\n",
      "Entrenando Gradient Boosting con n_estimators=25, learning_rate=0.01, max_depth=5\n",
      "Entrenando Gradient Boosting con n_estimators=25, learning_rate=0.1, max_depth=3\n",
      "Entrenando Gradient Boosting con n_estimators=25, learning_rate=0.1, max_depth=4\n",
      "Entrenando Gradient Boosting con n_estimators=25, learning_rate=0.1, max_depth=5\n",
      "Entrenando Gradient Boosting con n_estimators=25, learning_rate=0.2, max_depth=3\n",
      "Entrenando Gradient Boosting con n_estimators=25, learning_rate=0.2, max_depth=4\n",
      "Entrenando Gradient Boosting con n_estimators=25, learning_rate=0.2, max_depth=5\n",
      "Entrenando Gradient Boosting con n_estimators=50, learning_rate=0.01, max_depth=3\n",
      "Entrenando Gradient Boosting con n_estimators=50, learning_rate=0.01, max_depth=4\n",
      "Entrenando Gradient Boosting con n_estimators=50, learning_rate=0.01, max_depth=5\n",
      "Entrenando Gradient Boosting con n_estimators=50, learning_rate=0.1, max_depth=3\n",
      "Entrenando Gradient Boosting con n_estimators=50, learning_rate=0.1, max_depth=4\n",
      "Entrenando Gradient Boosting con n_estimators=50, learning_rate=0.1, max_depth=5\n",
      "Entrenando Gradient Boosting con n_estimators=50, learning_rate=0.2, max_depth=3\n",
      "Entrenando Gradient Boosting con n_estimators=50, learning_rate=0.2, max_depth=4\n",
      "Entrenando Gradient Boosting con n_estimators=50, learning_rate=0.2, max_depth=5\n",
      "Entrenando Gradient Boosting con n_estimators=100, learning_rate=0.01, max_depth=3\n",
      "Entrenando Gradient Boosting con n_estimators=100, learning_rate=0.01, max_depth=4\n",
      "Entrenando Gradient Boosting con n_estimators=100, learning_rate=0.01, max_depth=5\n",
      "Entrenando Gradient Boosting con n_estimators=100, learning_rate=0.1, max_depth=3\n",
      "Entrenando Gradient Boosting con n_estimators=100, learning_rate=0.1, max_depth=4\n",
      "Entrenando Gradient Boosting con n_estimators=100, learning_rate=0.1, max_depth=5\n",
      "Entrenando Gradient Boosting con n_estimators=100, learning_rate=0.2, max_depth=3\n",
      "Entrenando Gradient Boosting con n_estimators=100, learning_rate=0.2, max_depth=4\n",
      "Entrenando Gradient Boosting con n_estimators=100, learning_rate=0.2, max_depth=5\n",
      "Entrenando Gradient Boosting con n_estimators=150, learning_rate=0.01, max_depth=3\n",
      "Entrenando Gradient Boosting con n_estimators=150, learning_rate=0.01, max_depth=4\n",
      "Entrenando Gradient Boosting con n_estimators=150, learning_rate=0.01, max_depth=5\n",
      "Entrenando Gradient Boosting con n_estimators=150, learning_rate=0.1, max_depth=3\n",
      "Entrenando Gradient Boosting con n_estimators=150, learning_rate=0.1, max_depth=4\n",
      "Entrenando Gradient Boosting con n_estimators=150, learning_rate=0.1, max_depth=5\n",
      "Entrenando Gradient Boosting con n_estimators=150, learning_rate=0.2, max_depth=3\n",
      "Entrenando Gradient Boosting con n_estimators=150, learning_rate=0.2, max_depth=4\n",
      "Entrenando Gradient Boosting con n_estimators=150, learning_rate=0.2, max_depth=5\n",
      "Entrenando Gradient Boosting con n_estimators=200, learning_rate=0.01, max_depth=3\n",
      "Entrenando Gradient Boosting con n_estimators=200, learning_rate=0.01, max_depth=4\n",
      "Entrenando Gradient Boosting con n_estimators=200, learning_rate=0.01, max_depth=5\n",
      "Entrenando Gradient Boosting con n_estimators=200, learning_rate=0.1, max_depth=3\n",
      "Entrenando Gradient Boosting con n_estimators=200, learning_rate=0.1, max_depth=4\n",
      "Entrenando Gradient Boosting con n_estimators=200, learning_rate=0.1, max_depth=5\n",
      "Entrenando Gradient Boosting con n_estimators=200, learning_rate=0.2, max_depth=3\n",
      "Entrenando Gradient Boosting con n_estimators=200, learning_rate=0.2, max_depth=4\n",
      "Entrenando Gradient Boosting con n_estimators=200, learning_rate=0.2, max_depth=5\n"
     ]
    }
   ],
   "source": [
    "n_estimators_values =  [10, 25, 50, 100, 150, 200] # Número de estimadores\n",
    "learning_rate_values = [0.01, 0.1, 0.2]  # Tasa de aprendizaje\n",
    "max_depth_values = [3, 4, 5]  # Profundidad máxima de los árboles\n",
    "results = []\n",
    "\n",
    "for n_estimators in n_estimators_values:\n",
    "    for learning_rate in learning_rate_values:\n",
    "        for max_depth in max_depth_values:\n",
    "            print(f\"Entrenando Gradient Boosting con n_estimators={n_estimators}, learning_rate={learning_rate}, max_depth={max_depth}\")\n",
    "            gb_model = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth, random_state=42)\n",
    "            \n",
    "            results.append({\n",
    "                'n_estimators': n_estimators,\n",
    "                'learning_rate': learning_rate,\n",
    "                'max_depth': max_depth,\n",
    "                \"Accuracy\": cross_val_score(knn_model, train_data, y_train, cv=10, scoring=\"accuracy\").mean(),\n",
    "                \"Recall\": cross_val_score(knn_model, train_data, y_train, cv=10, scoring=\"recall\").mean(),\n",
    "                \"Precision\": cross_val_score(knn_model, train_data, y_train, cv=10, scoring=\"precision\").mean(),\n",
    "                \"F1\": cross_val_score(knn_model, train_data, y_train, cv=10, scoring=\"f1\").mean()\n",
    "            })\n",
    "\n",
    "results_gbc = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>3</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>4</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>25</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>3</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>4</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>0.20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>50</td>\n",
       "      <td>0.20</td>\n",
       "      <td>3</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>50</td>\n",
       "      <td>0.20</td>\n",
       "      <td>4</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>50</td>\n",
       "      <td>0.20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>100</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>100</td>\n",
       "      <td>0.10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>100</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>100</td>\n",
       "      <td>0.20</td>\n",
       "      <td>3</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>100</td>\n",
       "      <td>0.20</td>\n",
       "      <td>4</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>100</td>\n",
       "      <td>0.20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>150</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>150</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>150</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>150</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>150</td>\n",
       "      <td>0.10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>150</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>150</td>\n",
       "      <td>0.20</td>\n",
       "      <td>3</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>150</td>\n",
       "      <td>0.20</td>\n",
       "      <td>4</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>150</td>\n",
       "      <td>0.20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>200</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>200</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>200</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>200</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>200</td>\n",
       "      <td>0.10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>200</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>200</td>\n",
       "      <td>0.20</td>\n",
       "      <td>3</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>200</td>\n",
       "      <td>0.20</td>\n",
       "      <td>4</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>200</td>\n",
       "      <td>0.20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimators  learning_rate  max_depth  Accuracy    Recall  Precision  \\\n",
       "0             10           0.01          3  0.493584  0.465865   0.484838   \n",
       "1             10           0.01          4  0.493584  0.465865   0.484838   \n",
       "2             10           0.01          5  0.493584  0.465865   0.484838   \n",
       "3             10           0.10          3  0.493584  0.465865   0.484838   \n",
       "4             10           0.10          4  0.493584  0.465865   0.484838   \n",
       "5             10           0.10          5  0.493584  0.465865   0.484838   \n",
       "6             10           0.20          3  0.493584  0.465865   0.484838   \n",
       "7             10           0.20          4  0.493584  0.465865   0.484838   \n",
       "8             10           0.20          5  0.493584  0.465865   0.484838   \n",
       "9             25           0.01          3  0.493584  0.465865   0.484838   \n",
       "10            25           0.01          4  0.493584  0.465865   0.484838   \n",
       "11            25           0.01          5  0.493584  0.465865   0.484838   \n",
       "12            25           0.10          3  0.493584  0.465865   0.484838   \n",
       "13            25           0.10          4  0.493584  0.465865   0.484838   \n",
       "14            25           0.10          5  0.493584  0.465865   0.484838   \n",
       "15            25           0.20          3  0.493584  0.465865   0.484838   \n",
       "16            25           0.20          4  0.493584  0.465865   0.484838   \n",
       "17            25           0.20          5  0.493584  0.465865   0.484838   \n",
       "18            50           0.01          3  0.493584  0.465865   0.484838   \n",
       "19            50           0.01          4  0.493584  0.465865   0.484838   \n",
       "20            50           0.01          5  0.493584  0.465865   0.484838   \n",
       "21            50           0.10          3  0.493584  0.465865   0.484838   \n",
       "22            50           0.10          4  0.493584  0.465865   0.484838   \n",
       "23            50           0.10          5  0.493584  0.465865   0.484838   \n",
       "24            50           0.20          3  0.493584  0.465865   0.484838   \n",
       "25            50           0.20          4  0.493584  0.465865   0.484838   \n",
       "26            50           0.20          5  0.493584  0.465865   0.484838   \n",
       "27           100           0.01          3  0.493584  0.465865   0.484838   \n",
       "28           100           0.01          4  0.493584  0.465865   0.484838   \n",
       "29           100           0.01          5  0.493584  0.465865   0.484838   \n",
       "30           100           0.10          3  0.493584  0.465865   0.484838   \n",
       "31           100           0.10          4  0.493584  0.465865   0.484838   \n",
       "32           100           0.10          5  0.493584  0.465865   0.484838   \n",
       "33           100           0.20          3  0.493584  0.465865   0.484838   \n",
       "34           100           0.20          4  0.493584  0.465865   0.484838   \n",
       "35           100           0.20          5  0.493584  0.465865   0.484838   \n",
       "36           150           0.01          3  0.493584  0.465865   0.484838   \n",
       "37           150           0.01          4  0.493584  0.465865   0.484838   \n",
       "38           150           0.01          5  0.493584  0.465865   0.484838   \n",
       "39           150           0.10          3  0.493584  0.465865   0.484838   \n",
       "40           150           0.10          4  0.493584  0.465865   0.484838   \n",
       "41           150           0.10          5  0.493584  0.465865   0.484838   \n",
       "42           150           0.20          3  0.493584  0.465865   0.484838   \n",
       "43           150           0.20          4  0.493584  0.465865   0.484838   \n",
       "44           150           0.20          5  0.493584  0.465865   0.484838   \n",
       "45           200           0.01          3  0.493584  0.465865   0.484838   \n",
       "46           200           0.01          4  0.493584  0.465865   0.484838   \n",
       "47           200           0.01          5  0.493584  0.465865   0.484838   \n",
       "48           200           0.10          3  0.493584  0.465865   0.484838   \n",
       "49           200           0.10          4  0.493584  0.465865   0.484838   \n",
       "50           200           0.10          5  0.493584  0.465865   0.484838   \n",
       "51           200           0.20          3  0.493584  0.465865   0.484838   \n",
       "52           200           0.20          4  0.493584  0.465865   0.484838   \n",
       "53           200           0.20          5  0.493584  0.465865   0.484838   \n",
       "\n",
       "          F1  \n",
       "0   0.474805  \n",
       "1   0.474805  \n",
       "2   0.474805  \n",
       "3   0.474805  \n",
       "4   0.474805  \n",
       "5   0.474805  \n",
       "6   0.474805  \n",
       "7   0.474805  \n",
       "8   0.474805  \n",
       "9   0.474805  \n",
       "10  0.474805  \n",
       "11  0.474805  \n",
       "12  0.474805  \n",
       "13  0.474805  \n",
       "14  0.474805  \n",
       "15  0.474805  \n",
       "16  0.474805  \n",
       "17  0.474805  \n",
       "18  0.474805  \n",
       "19  0.474805  \n",
       "20  0.474805  \n",
       "21  0.474805  \n",
       "22  0.474805  \n",
       "23  0.474805  \n",
       "24  0.474805  \n",
       "25  0.474805  \n",
       "26  0.474805  \n",
       "27  0.474805  \n",
       "28  0.474805  \n",
       "29  0.474805  \n",
       "30  0.474805  \n",
       "31  0.474805  \n",
       "32  0.474805  \n",
       "33  0.474805  \n",
       "34  0.474805  \n",
       "35  0.474805  \n",
       "36  0.474805  \n",
       "37  0.474805  \n",
       "38  0.474805  \n",
       "39  0.474805  \n",
       "40  0.474805  \n",
       "41  0.474805  \n",
       "42  0.474805  \n",
       "43  0.474805  \n",
       "44  0.474805  \n",
       "45  0.474805  \n",
       "46  0.474805  \n",
       "47  0.474805  \n",
       "48  0.474805  \n",
       "49  0.474805  \n",
       "50  0.474805  \n",
       "51  0.474805  \n",
       "52  0.474805  \n",
       "53  0.474805  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_gbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hiperparámetros ExtraTree\n",
    "- n_estimators, default=100\n",
    "- criterion{“gini”, “entropy”, “log_loss”}, default=”gini”\n",
    "- max_depth, default=None\n",
    "- min_samples_split, default=2\n",
    "- min_samples_leaf, default=1\n",
    "- min_weight_fraction_leaf, default=0.0\n",
    "- max_features{“sqrt”, “log2”, None} default=”sqrt”\n",
    "- max_leaf_nodes, default=None\n",
    "- min_impurity_decrease, default=0.0\n",
    "- bootstrap, default=False\n",
    "- oob_score, default=False\n",
    "- random_state, RandomState instance or None, default=None\n",
    "- verbose, default=0\n",
    "- warm_start, default=False\n",
    "- class_weight{“balanced”, “balanced_subsample”}, default=None\n",
    "- ccp_alphanon-negative, default=0.0\n",
    "- max_samples, default=None\n",
    "- monotonic_cstarray-like, default=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando ExtraTree con n_estimators=10 criterion=gini, max_depth=3\n",
      "Entrenando ExtraTree con n_estimators=10 criterion=gini, max_depth=4\n",
      "Entrenando ExtraTree con n_estimators=10 criterion=gini, max_depth=5\n",
      "Entrenando ExtraTree con n_estimators=10 criterion=gini, max_depth=7\n",
      "Entrenando ExtraTree con n_estimators=10 criterion=gini, max_depth=9\n",
      "Entrenando ExtraTree con n_estimators=10 criterion=gini, max_depth=11\n",
      "Entrenando ExtraTree con n_estimators=10 criterion=gini, max_depth=15\n",
      "Entrenando ExtraTree con n_estimators=10 criterion=entropy, max_depth=3\n",
      "Entrenando ExtraTree con n_estimators=10 criterion=entropy, max_depth=4\n",
      "Entrenando ExtraTree con n_estimators=10 criterion=entropy, max_depth=5\n",
      "Entrenando ExtraTree con n_estimators=10 criterion=entropy, max_depth=7\n",
      "Entrenando ExtraTree con n_estimators=10 criterion=entropy, max_depth=9\n",
      "Entrenando ExtraTree con n_estimators=10 criterion=entropy, max_depth=11\n",
      "Entrenando ExtraTree con n_estimators=10 criterion=entropy, max_depth=15\n",
      "Entrenando ExtraTree con n_estimators=10 criterion=log_loss, max_depth=3\n",
      "Entrenando ExtraTree con n_estimators=10 criterion=log_loss, max_depth=4\n",
      "Entrenando ExtraTree con n_estimators=10 criterion=log_loss, max_depth=5\n",
      "Entrenando ExtraTree con n_estimators=10 criterion=log_loss, max_depth=7\n",
      "Entrenando ExtraTree con n_estimators=10 criterion=log_loss, max_depth=9\n",
      "Entrenando ExtraTree con n_estimators=10 criterion=log_loss, max_depth=11\n",
      "Entrenando ExtraTree con n_estimators=10 criterion=log_loss, max_depth=15\n",
      "Entrenando ExtraTree con n_estimators=25 criterion=gini, max_depth=3\n",
      "Entrenando ExtraTree con n_estimators=25 criterion=gini, max_depth=4\n",
      "Entrenando ExtraTree con n_estimators=25 criterion=gini, max_depth=5\n",
      "Entrenando ExtraTree con n_estimators=25 criterion=gini, max_depth=7\n",
      "Entrenando ExtraTree con n_estimators=25 criterion=gini, max_depth=9\n",
      "Entrenando ExtraTree con n_estimators=25 criterion=gini, max_depth=11\n",
      "Entrenando ExtraTree con n_estimators=25 criterion=gini, max_depth=15\n",
      "Entrenando ExtraTree con n_estimators=25 criterion=entropy, max_depth=3\n",
      "Entrenando ExtraTree con n_estimators=25 criterion=entropy, max_depth=4\n",
      "Entrenando ExtraTree con n_estimators=25 criterion=entropy, max_depth=5\n",
      "Entrenando ExtraTree con n_estimators=25 criterion=entropy, max_depth=7\n",
      "Entrenando ExtraTree con n_estimators=25 criterion=entropy, max_depth=9\n",
      "Entrenando ExtraTree con n_estimators=25 criterion=entropy, max_depth=11\n",
      "Entrenando ExtraTree con n_estimators=25 criterion=entropy, max_depth=15\n",
      "Entrenando ExtraTree con n_estimators=25 criterion=log_loss, max_depth=3\n",
      "Entrenando ExtraTree con n_estimators=25 criterion=log_loss, max_depth=4\n",
      "Entrenando ExtraTree con n_estimators=25 criterion=log_loss, max_depth=5\n",
      "Entrenando ExtraTree con n_estimators=25 criterion=log_loss, max_depth=7\n",
      "Entrenando ExtraTree con n_estimators=25 criterion=log_loss, max_depth=9\n",
      "Entrenando ExtraTree con n_estimators=25 criterion=log_loss, max_depth=11\n",
      "Entrenando ExtraTree con n_estimators=25 criterion=log_loss, max_depth=15\n",
      "Entrenando ExtraTree con n_estimators=50 criterion=gini, max_depth=3\n",
      "Entrenando ExtraTree con n_estimators=50 criterion=gini, max_depth=4\n",
      "Entrenando ExtraTree con n_estimators=50 criterion=gini, max_depth=5\n",
      "Entrenando ExtraTree con n_estimators=50 criterion=gini, max_depth=7\n",
      "Entrenando ExtraTree con n_estimators=50 criterion=gini, max_depth=9\n",
      "Entrenando ExtraTree con n_estimators=50 criterion=gini, max_depth=11\n",
      "Entrenando ExtraTree con n_estimators=50 criterion=gini, max_depth=15\n",
      "Entrenando ExtraTree con n_estimators=50 criterion=entropy, max_depth=3\n",
      "Entrenando ExtraTree con n_estimators=50 criterion=entropy, max_depth=4\n",
      "Entrenando ExtraTree con n_estimators=50 criterion=entropy, max_depth=5\n",
      "Entrenando ExtraTree con n_estimators=50 criterion=entropy, max_depth=7\n",
      "Entrenando ExtraTree con n_estimators=50 criterion=entropy, max_depth=9\n",
      "Entrenando ExtraTree con n_estimators=50 criterion=entropy, max_depth=11\n",
      "Entrenando ExtraTree con n_estimators=50 criterion=entropy, max_depth=15\n",
      "Entrenando ExtraTree con n_estimators=50 criterion=log_loss, max_depth=3\n",
      "Entrenando ExtraTree con n_estimators=50 criterion=log_loss, max_depth=4\n",
      "Entrenando ExtraTree con n_estimators=50 criterion=log_loss, max_depth=5\n",
      "Entrenando ExtraTree con n_estimators=50 criterion=log_loss, max_depth=7\n",
      "Entrenando ExtraTree con n_estimators=50 criterion=log_loss, max_depth=9\n",
      "Entrenando ExtraTree con n_estimators=50 criterion=log_loss, max_depth=11\n",
      "Entrenando ExtraTree con n_estimators=50 criterion=log_loss, max_depth=15\n",
      "Entrenando ExtraTree con n_estimators=100 criterion=gini, max_depth=3\n",
      "Entrenando ExtraTree con n_estimators=100 criterion=gini, max_depth=4\n",
      "Entrenando ExtraTree con n_estimators=100 criterion=gini, max_depth=5\n",
      "Entrenando ExtraTree con n_estimators=100 criterion=gini, max_depth=7\n",
      "Entrenando ExtraTree con n_estimators=100 criterion=gini, max_depth=9\n",
      "Entrenando ExtraTree con n_estimators=100 criterion=gini, max_depth=11\n",
      "Entrenando ExtraTree con n_estimators=100 criterion=gini, max_depth=15\n",
      "Entrenando ExtraTree con n_estimators=100 criterion=entropy, max_depth=3\n",
      "Entrenando ExtraTree con n_estimators=100 criterion=entropy, max_depth=4\n",
      "Entrenando ExtraTree con n_estimators=100 criterion=entropy, max_depth=5\n",
      "Entrenando ExtraTree con n_estimators=100 criterion=entropy, max_depth=7\n",
      "Entrenando ExtraTree con n_estimators=100 criterion=entropy, max_depth=9\n",
      "Entrenando ExtraTree con n_estimators=100 criterion=entropy, max_depth=11\n",
      "Entrenando ExtraTree con n_estimators=100 criterion=entropy, max_depth=15\n",
      "Entrenando ExtraTree con n_estimators=100 criterion=log_loss, max_depth=3\n",
      "Entrenando ExtraTree con n_estimators=100 criterion=log_loss, max_depth=4\n",
      "Entrenando ExtraTree con n_estimators=100 criterion=log_loss, max_depth=5\n",
      "Entrenando ExtraTree con n_estimators=100 criterion=log_loss, max_depth=7\n",
      "Entrenando ExtraTree con n_estimators=100 criterion=log_loss, max_depth=9\n",
      "Entrenando ExtraTree con n_estimators=100 criterion=log_loss, max_depth=11\n",
      "Entrenando ExtraTree con n_estimators=100 criterion=log_loss, max_depth=15\n",
      "Entrenando ExtraTree con n_estimators=150 criterion=gini, max_depth=3\n",
      "Entrenando ExtraTree con n_estimators=150 criterion=gini, max_depth=4\n",
      "Entrenando ExtraTree con n_estimators=150 criterion=gini, max_depth=5\n",
      "Entrenando ExtraTree con n_estimators=150 criterion=gini, max_depth=7\n",
      "Entrenando ExtraTree con n_estimators=150 criterion=gini, max_depth=9\n",
      "Entrenando ExtraTree con n_estimators=150 criterion=gini, max_depth=11\n",
      "Entrenando ExtraTree con n_estimators=150 criterion=gini, max_depth=15\n",
      "Entrenando ExtraTree con n_estimators=150 criterion=entropy, max_depth=3\n",
      "Entrenando ExtraTree con n_estimators=150 criterion=entropy, max_depth=4\n",
      "Entrenando ExtraTree con n_estimators=150 criterion=entropy, max_depth=5\n",
      "Entrenando ExtraTree con n_estimators=150 criterion=entropy, max_depth=7\n",
      "Entrenando ExtraTree con n_estimators=150 criterion=entropy, max_depth=9\n",
      "Entrenando ExtraTree con n_estimators=150 criterion=entropy, max_depth=11\n",
      "Entrenando ExtraTree con n_estimators=150 criterion=entropy, max_depth=15\n",
      "Entrenando ExtraTree con n_estimators=150 criterion=log_loss, max_depth=3\n",
      "Entrenando ExtraTree con n_estimators=150 criterion=log_loss, max_depth=4\n",
      "Entrenando ExtraTree con n_estimators=150 criterion=log_loss, max_depth=5\n",
      "Entrenando ExtraTree con n_estimators=150 criterion=log_loss, max_depth=7\n",
      "Entrenando ExtraTree con n_estimators=150 criterion=log_loss, max_depth=9\n",
      "Entrenando ExtraTree con n_estimators=150 criterion=log_loss, max_depth=11\n",
      "Entrenando ExtraTree con n_estimators=150 criterion=log_loss, max_depth=15\n",
      "Entrenando ExtraTree con n_estimators=200 criterion=gini, max_depth=3\n",
      "Entrenando ExtraTree con n_estimators=200 criterion=gini, max_depth=4\n",
      "Entrenando ExtraTree con n_estimators=200 criterion=gini, max_depth=5\n",
      "Entrenando ExtraTree con n_estimators=200 criterion=gini, max_depth=7\n",
      "Entrenando ExtraTree con n_estimators=200 criterion=gini, max_depth=9\n",
      "Entrenando ExtraTree con n_estimators=200 criterion=gini, max_depth=11\n",
      "Entrenando ExtraTree con n_estimators=200 criterion=gini, max_depth=15\n",
      "Entrenando ExtraTree con n_estimators=200 criterion=entropy, max_depth=3\n",
      "Entrenando ExtraTree con n_estimators=200 criterion=entropy, max_depth=4\n",
      "Entrenando ExtraTree con n_estimators=200 criterion=entropy, max_depth=5\n",
      "Entrenando ExtraTree con n_estimators=200 criterion=entropy, max_depth=7\n",
      "Entrenando ExtraTree con n_estimators=200 criterion=entropy, max_depth=9\n",
      "Entrenando ExtraTree con n_estimators=200 criterion=entropy, max_depth=11\n",
      "Entrenando ExtraTree con n_estimators=200 criterion=entropy, max_depth=15\n",
      "Entrenando ExtraTree con n_estimators=200 criterion=log_loss, max_depth=3\n",
      "Entrenando ExtraTree con n_estimators=200 criterion=log_loss, max_depth=4\n",
      "Entrenando ExtraTree con n_estimators=200 criterion=log_loss, max_depth=5\n",
      "Entrenando ExtraTree con n_estimators=200 criterion=log_loss, max_depth=7\n",
      "Entrenando ExtraTree con n_estimators=200 criterion=log_loss, max_depth=9\n",
      "Entrenando ExtraTree con n_estimators=200 criterion=log_loss, max_depth=11\n",
      "Entrenando ExtraTree con n_estimators=200 criterion=log_loss, max_depth=15\n"
     ]
    }
   ],
   "source": [
    "n_estimators_values = [10, 25, 50, 100, 150, 200]  # Número de estimadores\n",
    "quality_type = ['gini', 'entropy', 'log_loss']\n",
    "max_depth_values = [3, 4, 5, 7, 9, 11, 15] \n",
    "results = []\n",
    "\n",
    "for n in n_estimators_values:\n",
    "    for quality in quality_type:\n",
    "        for value in max_depth_values:\n",
    "            print(f\"Entrenando ExtraTree con n_estimators={n} criterion={quality}, max_depth={value}\")\n",
    "            et_model = ExtraTreesClassifier(n_estimators=n, criterion=quality, max_depth=value)\n",
    "\n",
    "            results.append({\n",
    "                'n_estimators': quality,\n",
    "                'criterion': splitter,\n",
    "                'max_depth': value,\n",
    "                \"Accuracy\": cross_val_score(knn_model, train_data, y_train, cv=10, scoring=\"accuracy\").mean(),\n",
    "                \"Recall\": cross_val_score(knn_model, train_data, y_train, cv=10, scoring=\"recall\").mean(),\n",
    "                \"Precision\": cross_val_score(knn_model, train_data, y_train, cv=10, scoring=\"precision\").mean(),\n",
    "                \"F1\": cross_val_score(knn_model, train_data, y_train, cv=10, scoring=\"f1\").mean()\n",
    "            })\n",
    "\n",
    "results_etc = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>criterion</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gini</td>\n",
       "      <td>random</td>\n",
       "      <td>3</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gini</td>\n",
       "      <td>random</td>\n",
       "      <td>4</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gini</td>\n",
       "      <td>random</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gini</td>\n",
       "      <td>random</td>\n",
       "      <td>7</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gini</td>\n",
       "      <td>random</td>\n",
       "      <td>9</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>random</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>random</td>\n",
       "      <td>7</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>random</td>\n",
       "      <td>9</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>random</td>\n",
       "      <td>11</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>log_loss</td>\n",
       "      <td>random</td>\n",
       "      <td>15</td>\n",
       "      <td>0.493584</td>\n",
       "      <td>0.465865</td>\n",
       "      <td>0.484838</td>\n",
       "      <td>0.474805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimators criterion  max_depth  Accuracy    Recall  Precision        F1\n",
       "0           gini    random          3  0.493584  0.465865   0.484838  0.474805\n",
       "1           gini    random          4  0.493584  0.465865   0.484838  0.474805\n",
       "2           gini    random          5  0.493584  0.465865   0.484838  0.474805\n",
       "3           gini    random          7  0.493584  0.465865   0.484838  0.474805\n",
       "4           gini    random          9  0.493584  0.465865   0.484838  0.474805\n",
       "..           ...       ...        ...       ...       ...        ...       ...\n",
       "121     log_loss    random          5  0.493584  0.465865   0.484838  0.474805\n",
       "122     log_loss    random          7  0.493584  0.465865   0.484838  0.474805\n",
       "123     log_loss    random          9  0.493584  0.465865   0.484838  0.474805\n",
       "124     log_loss    random         11  0.493584  0.465865   0.484838  0.474805\n",
       "125     log_loss    random         15  0.493584  0.465865   0.484838  0.474805\n",
       "\n",
       "[126 rows x 7 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test estadistico para KNN\n",
    "Si p-value es pequeño (generalmente < 0.05), indica que hay una diferencia significativa en el rendimiento entre las configuraciones de hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(data=results_knn, x='n_neighbors', y='Recall', hue='weights', style='metric')\n",
    "plt.title('Recall en KNN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 2.3449164353569394, P-value: 0.0564558816131241\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "n_neighbors_values = [2, 3, 5, 7, 9, 11, 15, 20]  # Número de vecinos\n",
    "\n",
    "# Comparar accuracy entre diferentes configuraciones de n_neighbors\n",
    "accuracy_2_neighbors = results_knn[results_knn['n_neighbors'] == 2]['Recall']\n",
    "accuracy_3_neighbors = results_knn[results_knn['n_neighbors'] == 3]['Recall']\n",
    "accuracy_5_neighbors = results_knn[results_knn['n_neighbors'] == 5]['Recall']\n",
    "accuracy_7_neighbors = results_knn[results_knn['n_neighbors'] == 7]['Recall']\n",
    "accuracy_9_neighbors = results_knn[results_knn['n_neighbors'] == 9]['Recall']\n",
    "accuracy_11_neighbors = results_knn[results_knn['n_neighbors'] == 11]['Recall']\n",
    "accuracy_15_neighbors = results_knn[results_knn['n_neighbors'] == 15]['Recall']\n",
    "accuracy_20_neighbors = results_knn[results_knn['n_neighbors'] == 20]['Recall']\n",
    "\n",
    "# Prueba ANOVA\n",
    "f_statistic, p_value = f_oneway(accuracy_2_neighbors, accuracy_3_neighbors, accuracy_5_neighbors, \n",
    "                                accuracy_7_neighbors, accuracy_9_neighbors, accuracy_11_neighbors,\n",
    "                                accuracy_15_neighbors, accuracy_20_neighbors)\n",
    "print(f\"F-statistic: {f_statistic}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "accuracies = pd.DataFrame({\n",
    "    'accuracy': pd.concat([accuracy_3_neighbors, accuracy_5_neighbors, accuracy_7_neighbors, accuracy_9_neighbors]),\n",
    "    'n_neighbors': ['3'] * len(accuracy_3_neighbors) + ['5'] * len(accuracy_5_neighbors) + ['7'] * len(accuracy_7_neighbors) + ['9'] * len(accuracy_9_neighbors)\n",
    "})\n",
    "\n",
    "# Test de Tukey\n",
    "# reject: Indica si se rechaza la hipótesis nula (que no hay diferencia entre los grupos). \n",
    "# True: hay una diferencia significativa; \n",
    "# False, no hay una diferencia significativa.\n",
    "\n",
    "tukey_result = pairwise_tukeyhsd(endog=accuracies['accuracy'], groups=accuracies['n_neighbors'], alpha=0.05)\n",
    "print(tukey_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=results_knn, x='n_neighbors', y='Test accuracy', hue='weights', style='metric')\n",
    "plt.title('Accuracy según n_neighbors y weights en KNN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test estadistico para gradient boosting\n",
    "Si p-value es pequeño (generalmente < 0.05), indica que hay una diferencia significativa en el rendimiento entre las configuraciones de hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Comparar accuracy entre diferentes configuraciones de n_neighbors\n",
    "accuracy_50_estimators = results_gradient_boosting[results_gradient_boosting['n_estimators'] == 50]['Test accuracy']\n",
    "accuracy_100_estimators = results_gradient_boosting[results_gradient_boosting['n_estimators'] == 100]['Test accuracy']\n",
    "accuracy_150_estimators = results_gradient_boosting[results_gradient_boosting['n_estimators'] == 150]['Test accuracy']\n",
    "\n",
    "# Prueba ANOVA\n",
    "f_statistic, p_value = f_oneway(accuracy_50_estimators, accuracy_100_estimators, accuracy_150_estimators)\n",
    "print(f\"F-statistic: {f_statistic}, P-value: {p_value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
